# Safetyâ€‘Aware Reinforcement Learning in Grid World

> A portfolio project demonstrating reinforcement learning, risk modelling, and neural networks applied to safe decisionâ€‘making.

This project combines **tabular Qâ€‘learning** with a **supervised dangerâ€‘prediction model** to create an agent that learns to navigate a hazardous environment while reasoning about risk. The work mirrors realâ€‘world problems such as autonomous navigation, healthcare decision support, and robotics safety.

---

## What This Project Demonstrates

* Endâ€‘toâ€‘end RL workflow: environment â†’ agent â†’ evaluation
* Experiment design & comparison of reward shaping
* Feature engineering for stateâ€‘action safety representation
* Neural network classification with imbalanced classes
* Writing clean, modular research code

**Skills:** Python Â· Reinforcement Learning Â· Neural Networks Â· Data Engineering Â· Experiment Analysis

---

## Problem Overview

A 10Ã—10 Grid World contains:

* Start at (0,0), goal at (9,9)
* 15 hazardous cells (terminal failure)
* Actions: UP / DOWN / LEFT / RIGHT

The agent must reach the goal while minimizing:

1. Number of steps
2. Visits to dangerous regions

Two stepâ€‘penalty settings were studied:

* Mild: âˆ’0.1
* Harsh: âˆ’1.0

---

## 1. Environment Design

**Key features**

* Deterministic transitions with boundary checks
* Configurable reward structure
* Text renderer for debugging
* Support for fixed or random safe starts

**Reward function**

* Goal: +10
* Hazard: âˆ’10
* Step: configurable penalty

---

## 2. Qâ€‘Learning Agent

**Algorithm**
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',Â·) âˆ’ Q(s,a)]

**Training setup**

* 2000 episodes
* Îµâ€‘greedy exploration (1.0 â†’ 0.05)
* Î± = 0.1, Î³ = 0.99

**Evaluation metrics**

* Episode reward
* Episode length
* Safety violations
* Success rate

**Insights**

* Mild penalty encouraged exploration and higher returns
* Harsh penalty produced riskâ€‘averse but inefficient policies
* Clear tradeâ€‘off between speed and safety

---

## 3. Danger Map & Dataset Engineering

To enable safety reasoning, a **distanceâ€‘toâ€‘hazard map** was generated using BFS.

**Risk labels**

* 0 â€“ Hazard
* 1 â€“ One step away
* 2 â€“ Two steps away
* 3 â€“ Safe region

**Feature vector (10D)**

* Current position (x,y)
* Oneâ€‘hot action
* Next position (x',y')
* Normalized danger distances

Dataset split: 70/15/15

---

## 4. Neural Network Risk Predictor

**Architecture**

* Dense 64 â†’ ReLU
* Dense 64 â†’ ReLU
* Softmax (4 classes)

**Outcome**

* Nearâ€‘perfect accuracy on unseen states
* Enables preâ€‘action safety screening

---

## 5. Safetyâ€‘Aware RL Concept

The classifier can act as a **shield**:

* Block actions predicted as highâ€‘risk
* Guide exploration without reward hacking

This mirrors techniques used in:

* Safe robotics
* Clinical decision support
* Autonomous vehicles

---

## â–¶ï¸ Run

safe_rl_shield.ipynb
complete_dataset.pkl
safety_shield.h5

---

## Future Improvements

* Double Qâ€‘learning / SARSA comparison
* Prioritized replay for rare hazards
* Costâ€‘constrained RL (CMDP)
* Explainable risk features

---

## ğŸ‘©â€ğŸ’» Author

**Linh**

